{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/Users/minhphuong/Downloads/TVs-all-merged.json', 'r') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Standardize 'inch' representations\n",
    "    text = re.sub(r\"(?i)\\b(inch|inches|”|''|-inch|\\\"|”)\\b\", \"inch\", text)\n",
    "\n",
    "    # Standardize 'hertz' (hz) representations\n",
    "    text = re.sub(r\"(?i)\\b(hertz|hz|-hz)\\b\", \"hz\", text)\n",
    "\n",
    "    # Standardize 'pounds' (lbs) representations\n",
    "    text = re.sub(r\"(?i)\\b(\\d+(\\.\\d+)?)\\s*(pounds|lbs|lb)\\b\", r\"\\1lbs\", text)\n",
    "\n",
    "    # Standardize 'year' and 'days' representations\n",
    "    text = re.sub(r\"(?i)\\b(\\d+)\\s*(year|years|yr|yrs)\\b\", r\"\\1year\", text)  # Convert to 'Nyear'\n",
    "    text = re.sub(r\"(?i)\\b(\\d+)\\s*(day|days)\\b\", r\"\\1days\", text)  # Convert to 'Ndays'\n",
    "\n",
    "    # Convert power values like '5W + 5W' to '10W'\n",
    "    matches = re.findall(r\"(\\d+(?:\\.\\d+)?)W\", text)\n",
    "    if matches:\n",
    "        total_power = sum(float(num) for num in matches)\n",
    "        text = re.sub(r\"(\\d+(?:\\.\\d+)?W(?:\\s*\\+\\s*\\d+(?:\\.\\d+)?W)*)\", f\"{int(total_power) if total_power.is_integer() else total_power}W\", text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove non-alphanumeric characters (except for units like 'inch', 'hz', and 'lbs')\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "\n",
    "    # Remove spaces and non-alphanumeric tokens before units (e.g., '23 inch' -> '23inch', '33.3 lbs' -> '33.3lbs')\n",
    "    text = re.sub(r\"(\\d+(\\.\\d+)?)\\s*(inch|hz|lbs|year|days)\", r\"\\1\\3\", text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Flattening and cleaning the hierarchical data structure\n",
    "def flatten_data(data):\n",
    "    products = []\n",
    "    for model_id, product_list in data.items():\n",
    "        for product in product_list:\n",
    "            # Add 'modelID' to the product dictionary (retain for evaluation)\n",
    "            product['modelID'] = model_id\n",
    "\n",
    "            # Clean the title of each product\n",
    "            if 'title' in product:\n",
    "                product['cleaned_title'] = clean_text(product['title'])\n",
    "            \n",
    "            # Clean each value in the key-value pairs under 'featuresMap' after sorting the keys\n",
    "            if 'featuresMap' in product:\n",
    "                cleaned_values = []\n",
    "                # Sort the keys of featuresMap to ensure a consistent order\n",
    "                for key in sorted(product['featuresMap']):\n",
    "                    value = product['featuresMap'][key]\n",
    "                    # # Handle \"Yes\" values by replacing with the key\n",
    "                    if value == \"Yes\":\n",
    "                        cleaned_value = clean_text(key)\n",
    "                        cleaned_values.append(cleaned_value)\n",
    "                    # Skip key-value pairs where value is \"No\" or \"0\"\n",
    "                    elif value != \"No\" and value != \"0\":\n",
    "                        cleaned_value = clean_text(str(value))  # Convert value to string and clean it\n",
    "                        cleaned_values.append(cleaned_value)\n",
    "                \n",
    "                # Add cleaned values to the product\n",
    "                product['cleaned_featuresMap'] = cleaned_values\n",
    "\n",
    "            # Append the cleaned product to the flattened list\n",
    "            products.append(product)\n",
    "    \n",
    "    return products\n",
    "\n",
    "\n",
    "\n",
    "flattened_products = flatten_data(data)\n",
    "cleaned_data = [(product['modelID'], product['cleaned_title'], product['cleaned_featuresMap'], product['shop']) for product in flattened_products]\n",
    "df_cleaned = pd.DataFrame(cleaned_data, columns=['modelID', 'cleaned_title', 'cleaned_featuresMap', 'shop'])\n",
    "df_cleaned['cleaned_featuresMap'] = df_cleaned['cleaned_featuresMap'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "\n",
    "# Ensure each value in `cleaned_featuresMap` is treated as a string before joining\n",
    "df_cleaned['combined_text'] = df_cleaned['cleaned_title'] + ' ' + df_cleaned['cleaned_featuresMap']\n",
    "\n",
    "\n",
    "# Remove all words that do not contain numbers from 'combined_text'\n",
    "df_cleaned['combined_text'] = df_cleaned['combined_text'].apply(lambda x: ' '.join(word for word in x.split() if re.search(r'\\d', word)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to remove words longer than 6 characters and duplicates\n",
    "def clean_combined_key_text(text):\n",
    "    words = text.split()\n",
    "    seen = set()\n",
    "    unique_words = []\n",
    "    for word in words:\n",
    "        if len(word) <= 6 and word not in seen:  # Check length and uniqueness\n",
    "            seen.add(word)\n",
    "            unique_words.append(word)\n",
    "    return ' '.join(unique_words)\n",
    "\n",
    "# Apply the function to the 'combined_key_text' column\n",
    "df_cleaned['combined_text'] = df_cleaned['combined_text'].apply(clean_combined_key_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from hashlib import sha1\n",
    "import math\n",
    "\n",
    "\n",
    "def get_shingles(text, char_ngram):\n",
    "    # Tokenize the text into words\n",
    "    tokens = text.split()\n",
    "    # Normalize by sorting tokens alphabetically\n",
    "    normalized_tokens = ' '.join(sorted(tokens))\n",
    "    # Generate token-based shingles\n",
    "    if char_ngram is None:  # Use word-level shingling if no n-gram size is provided\n",
    "        return {normalized_tokens}\n",
    "    # Generate character-level shingles\n",
    "    if len(normalized_tokens) < char_ngram:\n",
    "        return {normalized_tokens}\n",
    "    return set(normalized_tokens[head:head + char_ngram] for head in range(len(normalized_tokens) - char_ngram + 1))\n",
    "\n",
    "\n",
    "\n",
    "def compute_set_signature(shingle_set, hash_functions):\n",
    "    set_sig = []\n",
    "    for h_funct in hash_functions:\n",
    "        min_hash = math.inf\n",
    "        for el in shingle_set:\n",
    "            h = h_funct.get_hash_value(el)\n",
    "            if h < min_hash:\n",
    "                min_hash = h\n",
    "        # Ensure min_hash is updated correctly\n",
    "        if min_hash == math.inf:\n",
    "            raise ValueError(\"min_hash was not updated; check hash function and shingle set.\")\n",
    "        set_sig.append(min_hash)\n",
    "    return set_sig\n",
    "\n",
    "\n",
    "def get_signature_matrix(documents, k, hash_functions):\n",
    "    signature_matrix = []\n",
    "    for doc in documents:\n",
    "        shingles = get_shingles(doc, k)\n",
    "        signature = compute_set_signature(shingles, hash_functions)\n",
    "        signature_matrix.append(signature)\n",
    "    return signature_matrix\n",
    "\n",
    "\n",
    "\n",
    "class HashFunction:\n",
    "    def __init__(self, p=2147483647):  # Using a large prime number as `p`\n",
    "        self.a = random.randint(1, p - 1)  # Random value for `a` in range [1, p-1]\n",
    "        self.b = random.randint(0, p - 1)  # Random value for `b` in range [0, p-1]\n",
    "        self.p = p\n",
    "\n",
    "    def get_hash_value(self, x):\n",
    "        # Compute the hash value using the formula: h_a,b(x) = (a + b * x) % p\n",
    "        return (self.a + self.b * int(sha1(x.encode('utf8')).hexdigest(), 16)) % self.p\n",
    "\n",
    "def get_signature_matrix_bands(sig_matrix, bands_nr, r):\n",
    "    # r = int(sign_len / bands_nr)  # Number of rows per band\n",
    "    bands = {i: [] for i in range(bands_nr)}  # Initialize bands dictionary\n",
    "    for signature in sig_matrix:\n",
    "        for i in range(bands_nr):\n",
    "            idx = i * r\n",
    "            # band_tuple = tuple(signature[idx:idx + r])\n",
    "            # bands[i].append(band_tuple)\n",
    "            bands[i].append(' '.join(str(x) for x in signature[idx:idx+r]) )       \n",
    "    return bands\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_band_buckets(band, hash_function):\n",
    "    buckets = {}\n",
    "    for doc_id, band_values in enumerate(band):\n",
    "        # Convert the band tuple to a string to use with the custom hash function\n",
    "        band_str = ''.join(map(str, band_values))\n",
    "        band_hash = hash_function.get_hash_value(band_str)\n",
    "        if band_hash not in buckets:\n",
    "            buckets[band_hash] = [doc_id]\n",
    "        else:\n",
    "            buckets[band_hash].append(doc_id)\n",
    "    return buckets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_candidates_list(buckets):\n",
    "    candidates = set()\n",
    "    # Buckets is a dictionary containing key=bucket, value=list of doc_ids that hashed to the bucket\n",
    "    for bucket, candidate_list in buckets.items():\n",
    "        if len(candidate_list) > 1:\n",
    "            for i in range(0, len(candidate_list) - 1):\n",
    "                for j in range(i + 1, len(candidate_list)):  \n",
    "                    pair = tuple(sorted((candidate_list[i], candidate_list[j])))\n",
    "                    candidates.add(pair)\n",
    "    return candidates  # A set of couples, each couple is a candidate pair\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate candidate pairs using LSH then detect duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Bootstrapping with LSH implementation\n",
    "n_bootstraps = 5\n",
    "sign_len = 100  # Length of signature (number of rows)\n",
    "k = 5  # Shingle length\n",
    "num_hash_functions = sign_len  # Number of hash functions\n",
    "random_b_values = [1, 7, 14, 19, 21, 24, 26, 31, 38, 40, 45, 49, 53, 60, 64, 73, 80, 87, 91, 100]\n",
    "\n",
    "fractions_of_comparisons = []\n",
    "pair_completeness_values = []\n",
    "bands_rows_comparisons = []\n",
    "num_comparisons_values = []  # To store num_comparisons for each number of bands\n",
    "pair_quality_values = []  # To store pair quality for each number of bands\n",
    "f1_measure_values = []  # To store F1-measure for each number of bands\n",
    "f1star_measure_values = []  # To store F1* measure for each number of bands\n",
    "\n",
    "# To store b and r values for each configuration\n",
    "b_values = []\n",
    "r_values = []\n",
    "\n",
    "# Generate hash functions\n",
    "hash_functions = [HashFunction() for _ in range(num_hash_functions)]\n",
    "\n",
    "# Iterate over number of bands (b)\n",
    "for b in random_b_values:  # From 1 to 100 bands\n",
    "    r = max(1, sign_len // b)  # Calculate rows per band, ensuring at least 1 row per band\n",
    "\n",
    "    # Store `b` and `r` values for the current configuration\n",
    "    b_values.append(b)\n",
    "    r_values.append(r)\n",
    "\n",
    "    pair_completeness_bootstrap = []\n",
    "    fraction_comparisons_bootstrap = []\n",
    "    num_comparisons_per_bands = []  # Store num_comparisons for each bootstrap\n",
    "    pair_quality_bootstrap = []  # Store pair quality for each bootstrap\n",
    "    precision_bootstrap = []\n",
    "    recall_bootstrap = []\n",
    "\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "\n",
    "        # Bootstrap sample (100% of the data, with replacement)\n",
    "        sample = df_cleaned.sample(frac=1, replace=True, random_state=13 + _).reset_index(drop=True)\n",
    "\n",
    "        original_indices = sample.index.tolist()\n",
    "        sample['product_descript'] = sample.apply(lambda row: (row['shop'], row['cleaned_title']), axis=1)\n",
    "\n",
    "        # Identify unique products\n",
    "        unique_product_indices = []\n",
    "        seen_products = set()\n",
    "        for idx, product_id in enumerate(sample['product_descript']):\n",
    "            if product_id not in seen_products:\n",
    "                seen_products.add(product_id)\n",
    "                unique_product_indices.append(idx)\n",
    "\n",
    "        # Create unique_products DataFrame\n",
    "        bootstrap_df = sample.iloc[unique_product_indices].reset_index(drop=True)\n",
    "\n",
    "        # Generate signature matrix for unique products\n",
    "        unique_documents = bootstrap_df['combined_text'].tolist()\n",
    "        bootstrap_sig_matrix = get_signature_matrix(unique_documents, k, hash_functions)\n",
    "\n",
    "        # Generate bands\n",
    "        bands = get_signature_matrix_bands(bootstrap_sig_matrix, b, r)\n",
    "\n",
    "        # Generate candidate pairs for each band and aggregate them\n",
    "        candidate_pairs = set()\n",
    "\n",
    "        for band_id, band in enumerate(bands.values()):\n",
    "            # Select a hash function for this band (cyclic if there are more bands than hash functions)\n",
    "            hash_function = hash_functions[band_id % len(hash_functions)]\n",
    "            buckets = get_band_buckets(band, hash_function)\n",
    "            candidates = get_candidates_list(buckets)\n",
    "            candidate_pairs.update(candidates)\n",
    "\n",
    "        # Perform similarity checks on candidate pairs\n",
    "        detected_duplicates = []\n",
    "\n",
    "        # Load a pre-trained Sentence Transformer model\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Generate embeddings for all rows in the dataframe\n",
    "        texts = bootstrap_df['combined_text']\n",
    "        embeddings = model.encode(texts.values)\n",
    "\n",
    "        # Initialize an infinite dissimilarity matrix\n",
    "        # Initialize the dissimilarity matrix with a large value (10^8) for all non-candidate pairs\n",
    "        dissimilarity_matrix = np.full((len(bootstrap_df), len(bootstrap_df)), 10**8)\n",
    "\n",
    "\n",
    "        # Compute dissimilarities for candidate pairs\n",
    "        for pair in candidate_pairs:\n",
    "            idx1, idx2 = pair\n",
    "\n",
    "            # Extract combined text for both indices\n",
    "            text1 = bootstrap_df.loc[idx1, 'combined_text']\n",
    "            text2 = bootstrap_df.loc[idx2, 'combined_text']\n",
    "\n",
    "            # Compute similarity using embeddings\n",
    "            embedding1 = embeddings[list(bootstrap_df.index).index(idx1)]\n",
    "            embedding2 = embeddings[list(bootstrap_df.index).index(idx2)]\n",
    "            similarity = util.cos_sim(embedding1, embedding2).item()\n",
    "\n",
    "            # Convert similarity to dissimilarity\n",
    "            dissimilarity = 1 - similarity\n",
    "\n",
    "            # Update the dissimilarity matrix for the candidate pair\n",
    "            dissimilarity_matrix[idx1, idx2] = dissimilarity\n",
    "            dissimilarity_matrix[idx2, idx1] = dissimilarity  # Symmetric matrix\n",
    "\n",
    "        # Enforce constraints for same shop and different brands\n",
    "        for i in range(len(bootstrap_df)):\n",
    "            for j in range(i + 1, len(bootstrap_df)):\n",
    "                # Check if the pair does not meet conditions (e.g., different brands)\n",
    "                if (\n",
    "                    bootstrap_df.loc[i, 'shop'] == bootstrap_df.loc[j, 'shop'] and\n",
    "                    bootstrap_df.loc[i, 'Brand'] != bootstrap_df.loc[j, 'Brand']\n",
    "                ):\n",
    "                    dissimilarity_matrix[i, j] = 10**8\n",
    "                    dissimilarity_matrix[j, i] = 10**8  # Symmetric\n",
    "\n",
    "\n",
    "        # Ensure it's in condensed form for clustering\n",
    "        condensed_dissimilarity = squareform(dissimilarity_matrix, checks=False)\n",
    "\n",
    "        # Perform average linkage clustering\n",
    "        linkage_matrix = linkage(condensed_dissimilarity, method='average')\n",
    "\n",
    "        # Define a threshold for clustering (adjust as needed)\n",
    "        threshold = 0.6\n",
    "\n",
    "        # Extract cluster labels\n",
    "        clusters = fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "\n",
    "        # Assign clusters to the dataframe\n",
    "        bootstrap_df['cluster'] = clusters\n",
    "\n",
    "        # Identify duplicate pairs based on clusters\n",
    "        detected_duplicates = []\n",
    "        for i in range(len(bootstrap_df)):\n",
    "            for j in range(i + 1, len(bootstrap_df)):\n",
    "                if bootstrap_df.loc[i, 'cluster'] == bootstrap_df.loc[j, 'cluster']:\n",
    "                    detected_duplicates.append((i, j))\n",
    "\n",
    "\n",
    "        # Calculate total number of true duplicate pairs in the bootstrap dataset\n",
    "        true_duplicate_pairs = 0\n",
    "        duplicate_groups = bootstrap_df['modelID'].value_counts()\n",
    "        for count in duplicate_groups:\n",
    "            if count > 1:\n",
    "                true_duplicate_pairs += count * (count - 1) / 2\n",
    "\n",
    "        # Calculate metrics for the bootstrap sample\n",
    "        num_comparisons = len(candidate_pairs)\n",
    "        total_possible_comparisons = len(bootstrap_sig_matrix) * (len(bootstrap_sig_matrix) - 1) / 2\n",
    "        duplicates_found = len([pair for pair in detected_duplicates if bootstrap_df.loc[pair[0], 'modelID'] == bootstrap_df.loc[pair[1], 'modelID']])\n",
    "        pair_completeness = duplicates_found / true_duplicate_pairs if true_duplicate_pairs > 0 else 0\n",
    "        recall = pair_completeness\n",
    "        precision = duplicates_found / len(detected_duplicates) \n",
    "        fraction_of_comparisons = num_comparisons / total_possible_comparisons if total_possible_comparisons > 0 else 0\n",
    "        pair_quality = duplicates_found / num_comparisons if num_comparisons > 0 else 0\n",
    "\n",
    "        # Store metrics for the bootstrap\n",
    "        pair_completeness_bootstrap.append(pair_completeness)\n",
    "        fraction_comparisons_bootstrap.append(fraction_of_comparisons)\n",
    "        num_comparisons_per_bands.append(num_comparisons)\n",
    "        pair_quality_bootstrap.append(pair_quality)\n",
    "        precision_bootstrap.append(precision)\n",
    "        recall_bootstrap.append(recall)\n",
    "\n",
    "\n",
    "    # Average over all bootstraps\n",
    "    avg_pair_completeness = np.mean(pair_completeness_bootstrap)\n",
    "    avg_pair_quality = np.mean(pair_quality_bootstrap)\n",
    "    avg_precision = np.mean(precision_bootstrap)\n",
    "    avg_recall = np.mean(recall_bootstrap)\n",
    "\n",
    "    # Calculate F1-measure using the formula: F1 = 2 * PQ * PC / (PQ + PC)\n",
    "    f1_measure = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    f1star_measure = (2 * avg_pair_quality * avg_pair_completeness) / (avg_pair_quality + avg_pair_completeness) if (avg_pair_quality + avg_pair_completeness) > 0 else 0\n",
    "\n",
    "    # Store metrics for this number of bands\n",
    "    pair_completeness_values.append(avg_pair_completeness)\n",
    "    fractions_of_comparisons.append(np.mean(fraction_comparisons_bootstrap))\n",
    "    bands_rows_comparisons.append(np.mean(num_comparisons_per_bands))\n",
    "    num_comparisons_values.append(np.mean(num_comparisons_per_bands))\n",
    "    pair_quality_values.append(avg_pair_quality)\n",
    "    f1_measure_values.append(f1_measure)\n",
    "    f1star_measure_values.append(f1star_measure)\n",
    "\n",
    "# Print b, r, num_comparisons, pair quality, and F1-measure values for each configuration\n",
    "for i, b in enumerate(b_values):\n",
    "    print(f\"Number of Bands: {b}, Rows per Band: {r_values[i]}, num_comparisons: {num_comparisons_values[i]}, pair_quality: {pair_quality_values[i]}, F1-measure: {f1_measure_values[i]}, F1star-measure: {f1star_measure_values[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generalized plotting function for subplots\n",
    "def plot_metric_subplot(ax, fractions, values, xlabel, ylabel, title, xlim=None, ylim=None, color='b'):\n",
    "    ax.plot(fractions, values, marker='', linestyle='-', color=color)  # Smooth line\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    if xlim:\n",
    "        ax.set_xlim(xlim)\n",
    "    if ylim:\n",
    "        ax.set_ylim(ylim)\n",
    "    ax.grid(True)\n",
    "    ax.set_aspect('equal', adjustable='box')  # Make each subplot area square\n",
    "\n",
    "# Create subplots for 3 plots in 1 row\n",
    "fig, axs = plt.subplots(1, 4, figsize=(18, 6))  # 1 row, 3 columns\n",
    "\n",
    "# Plot Pair Completeness\n",
    "plot_metric_subplot(\n",
    "    axs[0],\n",
    "    fractions_of_comparisons,\n",
    "    pair_completeness_values,\n",
    "    xlabel='Fraction of Comparisons',\n",
    "    ylabel='Pair Completeness',\n",
    "    title='Pair Completeness',\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    color='b'\n",
    ")\n",
    "\n",
    "# Plot Pair Quality\n",
    "plot_metric_subplot(\n",
    "    axs[1],\n",
    "    fractions_of_comparisons,\n",
    "    pair_quality_values,\n",
    "    xlabel='Fraction of Comparisons',\n",
    "    ylabel='Pair Quality',\n",
    "    title='Pair Quality',\n",
    "    xlim=(0, 0.06),\n",
    "    ylim=(0, 0.06),\n",
    "    color='b'\n",
    ")\n",
    "\n",
    "# Plot F1-Measure\n",
    "plot_metric_subplot(\n",
    "    axs[2],\n",
    "    fractions_of_comparisons,\n",
    "    f1_measure_values,\n",
    "    xlabel='Fraction of Comparisons',\n",
    "    ylabel='F1-Measure',\n",
    "    title='F1-Measure',\n",
    "    xlim=(0, 0.04),\n",
    "    ylim=(0, 0.04),\n",
    "    color='b'\n",
    ")\n",
    "\n",
    "# Plot F1*-Measure\n",
    "plot_metric_subplot(\n",
    "    axs[3],\n",
    "    fractions_of_comparisons,\n",
    "    f1star_measure_values,\n",
    "    xlabel='Fraction of Comparisons',\n",
    "    ylabel='F1*-Measure',\n",
    "    title='F1*-Measure',\n",
    "    xlim=(0, 0.04),\n",
    "    ylim=(0, 0.04),\n",
    "    color='b'\n",
    ")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
